# ğŸ¯ Model Performance Monitor - START HERE

## Welcome!

This folder contains a complete **Model Performance Monitoring System** for production credit models in KNIME.

---

## ğŸ“š What's Inside?

### ğŸš€ **Core Implementation**

| File | Purpose | When to Use |
|------|---------|-------------|
| **`model_performance_monitor.py`** | Main production node | Add to your KNIME workflow |
| | ~1,300 lines of production-ready code | Connect production data & baseline |

### ğŸ“– **Documentation** (Read in This Order)

| # | File | Purpose | Read Time |
|---|------|---------|-----------|
| 1ï¸âƒ£ | **`README.md`** | Complete technical reference | 30 min |
| | | All metrics explained, flow variables, troubleshooting | |
| 2ï¸âƒ£ | **`BASELINE_STRATEGY.md`** | When to use training vs production baseline | 15 min |
| | | Critical for correct interpretation | |
| 3ï¸âƒ£ | **`USAGE_EXAMPLE.md`** | Real-world scenarios with examples | 20 min |
| | | Healthy model, drift, retraining decisions | |
| 4ï¸âƒ£ | **`KNIME_WORKFLOW_INTEGRATION_GUIDE.md`** | How to integrate with existing workflow | 25 min |
| | | Step-by-step integration patterns | |
| 5ï¸âƒ£ | **`TESTING_VALIDATION_GUIDE.md`** | Testing checklist before production | 20 min |
| | | Validate it works correctly | |

### ğŸ› ï¸ **Tools & Utilities**

| File | Purpose | When to Use |
|------|---------|-------------|
| **`test_data_generator.py`** | Generate synthetic test data | Testing the node |
| | Creates realistic loan data with configurable drift | Before using real data |
| **`monitor_config_generator.py`** | Preset threshold configurations | Quick setup |
| | 6 presets: conservative, standard, aggressive, etc. | Choose your monitoring style |

### ğŸ“Š **Additional Resources**

| File | Purpose |
|------|---------|
| **`IMPLEMENTATION_SUMMARY.md`** | Overview of what was built, status, next steps |

---

## âš¡ Quick Start (5 Minutes)

### Option 1: Test with Synthetic Data (Recommended First)

```
1. Open KNIME
2. Add Python Script node
3. Paste contents of test_data_generator.py
4. Execute (generates 2 output tables)
5. Add Model Performance Monitor node
6. Connect:
   - Generator Output 1 â†’ Monitor Input 1 (production)
   - Generator Output 0 â†’ Monitor Input 2 (baseline)
7. Set flow variable: DependentVariable = "isBad"
8. Execute Monitor node
9. View outputs!

Expected: Recommendation = "MONITOR" âš ï¸
(Test data has moderate drift configured)
```

### Option 2: Use Preset Configuration

```
1. Add Python Script node
2. Paste contents of monitor_config_generator.py
3. Set PRESET = "standard" at top of script
4. Execute (generates flow variable table)
5. Add "Table Row to Variable Loop Start"
6. Connect config generator â†’ loop start
7. Add Model Performance Monitor inside loop
8. Connect your data
9. Execute!

All thresholds automatically configured âœ“
```

---

## ğŸ“‹ What the Node Does

### Input

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Production Data         â”‚ â† Current period (e.g., last month)
â”‚                         â”‚
â”‚ Columns:                â”‚
â”‚  â€¢ score                â”‚ â† Scorecard points
â”‚  â€¢ isApproved           â”‚ â† 1=approved, 0=declined
â”‚  â€¢ isFunded             â”‚ â† 1=funded, 0=not funded
â”‚  â€¢ isBad (or other DV)  â”‚ â† Actual outcome (funded only)
â”‚  â€¢ ROI                  â”‚ â† Return on investment (funded only)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Baseline Data (optional)â”‚ â† Training or historical stable period
â”‚ Same columns as above   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Output

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Performance Summary                               â”‚
â”‚    Key metrics with alerts and recommendation       â”‚
â”‚                                                      â”‚
â”‚    Sample Size: 1,200  âœ“                            â”‚
â”‚    AUC: 0.72 (baseline: 0.76, -0.04)  âš ï¸            â”‚
â”‚    PSI: 0.18  âš ï¸                                     â”‚
â”‚    Bad Rate: 0.15 (baseline: 0.12, +0.03)  âš ï¸       â”‚
â”‚    Recommendation: MONITOR âš ï¸                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Decile Analysis                                   â”‚
â”‚    Performance by score decile (1-10)                â”‚
â”‚                                                      â”‚
â”‚    Decile 1: Score 800-900, Bad Rate 2%             â”‚
â”‚    Decile 5: Score 600-650, Bad Rate 15%            â”‚
â”‚    Decile 10: Score 300-400, Bad Rate 75%           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. Calibration Table                                 â”‚
â”‚    Expected vs observed rates by score band          â”‚
â”‚                                                      â”‚
â”‚    Bin [0.1-0.2): Predicted 15%, Observed 18%       â”‚
â”‚    Difference: +3%  â†’ Calibrated âœ“                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. Diagnostics Data                                  â”‚
â”‚    Original data + probability, decile, flags        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ Key Metrics Explained (30 Second Version)

| Metric | What It Measures | Good | Warning | Critical |
|--------|------------------|------|---------|----------|
| **PSI** | Population shift from baseline | < 0.1 | 0.1-0.25 | â‰¥ 0.25 |
| **AUC** | Model discrimination ability | â‰¥ 0.70 | Decline 3-5% | Decline > 5% |
| **K-S** | Separation of goods/bads | â‰¥ 0.35 | Decline 5-10% | Decline > 10% |
| **Bad Rate** | % defaults in funded | Stable | +2-3% | +5%+ |
| **Calibration** | Probability accuracy | < 0.05 | 0.05-0.10 | > 0.10 |

### Recommendation

- **OK âœ“**: Model healthy, continue normal operations
- **MONITOR âš ï¸**: Early warning signs, watch closely, investigate
- **RETRAIN ğŸ”´**: Model degraded significantly, retraining recommended

---

## ğŸ—ºï¸ Documentation Roadmap

### If You Have 10 Minutes

Read: **README.md (first 3 sections)**
- What the node does
- How to configure it
- Key metrics explained

### If You Have 30 Minutes

Read in order:
1. **README.md** (skip troubleshooting for now)
2. **USAGE_EXAMPLE.md** (Scenarios 1-3)

### If You Have 1 Hour

Add:
3. **BASELINE_STRATEGY.md** (understand baseline options)
4. **KNIME_WORKFLOW_INTEGRATION_GUIDE.md** (Scenarios 1-2)

### If You're Deploying to Production

Read everything:
1. README.md (complete)
2. BASELINE_STRATEGY.md (complete)
3. USAGE_EXAMPLE.md (complete)
4. KNIME_WORKFLOW_INTEGRATION_GUIDE.md (complete)
5. TESTING_VALIDATION_GUIDE.md (complete)
6. Run through tests before production use

---

## ğŸ’¡ Common Questions

### Q: Do I need baseline data?

**A:** Optional but recommended.
- **Without baseline:** Get absolute metrics (AUC, bad rate) but no drift detection (PSI)
- **With baseline:** Full comparative analysis, detect population shifts, better recommendations

### Q: What should I use as baseline?

**A:** Depends on timeline:
- **Months 1-6:** Use training data (only option available)
- **After Month 6:** Use rolling 6-12 month production baseline (more relevant)
- See `BASELINE_STRATEGY.md` for details

### Q: How often should I run this?

**A:** Depends on risk tolerance:
- **Monthly:** Standard for most applications
- **Quarterly:** For stable, mature models
- **Weekly:** For high-risk applications or volatile markets

### Q: What if recommendation is "RETRAIN"?

**A:** Don't panic! Follow this workflow:
1. **Verify**: Check if results make sense (not data quality issue)
2. **Investigate**: Why did metrics degrade? Population shift? External factors?
3. **Decide**: Consider business context, not just metrics
4. **Plan**: If retraining needed, gather last 12 months data
5. **Rebuild**: Loop back to development workflow
6. **Validate**: Test new model thoroughly before deployment

### Q: Can I adjust thresholds?

**A:** Yes! Two ways:
1. **Use presets:** Run `monitor_config_generator.py` with different PRESET
2. **Custom:** Set flow variables manually or use "custom" preset

Start with "standard", adjust after 3-6 months of experience.

### Q: How do scorecard parameters work?

**A:** The node converts scores to probabilities using:
- **Points** (default 600): Base score at target odds
- **Odds** (default 20): Odds ratio (1:19)
- **PDO** (default 50): Points to Double the Odds

**IMPORTANT:** Must match your Scorecard Generator settings!

---

## ğŸš€ Next Steps

### Today (Testing)
1. âœ… Generate test data with `test_data_generator.py`
2. âœ… Run Model Performance Monitor
3. âœ… Verify outputs look reasonable
4. âœ… Read README.md

### This Week (Learning)
5. âœ… Read USAGE_EXAMPLE.md scenarios
6. âœ… Read BASELINE_STRATEGY.md
7. âœ… Try different test scenarios (change drift levels)
8. âœ… Test with your own data (non-production)

### This Month (Integration)
9. âœ… Read KNIME_WORKFLOW_INTEGRATION_GUIDE.md
10. âœ… Integrate into your workflow
11. âœ… Set up flow variables
12. âœ… Configure preset thresholds

### Before Production (Validation)
13. âœ… Complete TESTING_VALIDATION_GUIDE.md checklist
14. âœ… Run with recent production data
15. âœ… Validate recommendations make sense
16. âœ… Document your configuration choices

### In Production (Operations)
17. âœ… Schedule monthly monitoring workflow
18. âœ… Set up automated reports
19. âœ… Track metrics over time
20. âœ… Refine thresholds based on experience

---

## ğŸ“ Support & Resources

### Getting Help

1. **Error message?** â†’ See README.md "Troubleshooting" section
2. **Integration issues?** â†’ See KNIME_WORKFLOW_INTEGRATION_GUIDE.md
3. **Metrics confusing?** â†’ See USAGE_EXAMPLE.md for interpretation
4. **Testing questions?** â†’ See TESTING_VALIDATION_GUIDE.md

### Additional Files

- **CONTEXT.md** (project root): Overview of all nodes in toolkit
- **COMPREHENSIVE_DEVELOPMENT_LOG.md** (project root): Project history

---

## ğŸ“Š File Summary

```
model_performance_monitor/
â”‚
â”œâ”€â”€ ğŸš€ CORE
â”‚   â”œâ”€â”€ model_performance_monitor.py      (1,300 lines - main node)
â”‚   â”œâ”€â”€ test_data_generator.py            (500 lines - test data)
â”‚   â””â”€â”€ monitor_config_generator.py       (600 lines - presets)
â”‚
â”œâ”€â”€ ğŸ“– DOCUMENTATION
â”‚   â”œâ”€â”€ START_HERE.md                     (this file - overview)
â”‚   â”œâ”€â”€ README.md                         (650 lines - complete reference)
â”‚   â”œâ”€â”€ BASELINE_STRATEGY.md              (350 lines - baseline guide)
â”‚   â”œâ”€â”€ USAGE_EXAMPLE.md                  (700 lines - real scenarios)
â”‚   â”œâ”€â”€ KNIME_WORKFLOW_INTEGRATION_GUIDE.md (900 lines - integration)
â”‚   â”œâ”€â”€ TESTING_VALIDATION_GUIDE.md       (800 lines - testing)
â”‚   â””â”€â”€ IMPLEMENTATION_SUMMARY.md         (400 lines - overview)
â”‚
â””â”€â”€ ğŸ“Š TOTAL: ~6,200 lines of code and documentation
```

---

## âœ… What You Have Now

### A Complete Model Monitoring System That:

- âœ… **Detects drift** before it becomes a problem (PSI)
- âœ… **Tracks performance** with industry-standard metrics (AUC, K-S)
- âœ… **Monitors calibration** (predicted vs observed rates)
- âœ… **Measures business impact** (approval rates, bad rates, ROI)
- âœ… **Provides clear recommendations** (OK / MONITOR / RETRAIN)
- âœ… **Integrates seamlessly** with your KNIME workflow
- âœ… **Includes test data** for validation
- âœ… **Has preset configurations** for quick setup
- âœ… **Is thoroughly documented** (6,200+ lines of docs)

---

## ğŸ‰ You're Ready!

Your model monitoring system is **production-ready**. Follow the Quick Start above to begin testing, then work through the documentation at your own pace.

**The node is designed to grow with your needs:**
- Start simple (just production data, default thresholds)
- Add baseline for drift detection
- Adjust thresholds based on experience
- Integrate into scheduled workflows
- Automate decision-making

**Questions?** Start with README.md, then explore the other guides.

**Ready to test?** Run the Quick Start above (5 minutes).

---

## ğŸ“ˆ Success Path

```
Day 1:     Test with synthetic data âœ“
Day 2-3:   Read documentation
Week 1:    Test with your own data
Week 2-3:  Integrate into workflow
Week 4:    Deploy to production
Month 2:   First real monitoring run
Month 3:   Refine thresholds
Month 6:   Establish rolling baseline
Month 12:  First potential retraining decision
```

---

**Welcome to production model monitoring! ğŸ¯**

**Your model's health is now quantified, tracked, and actionable.**

---

*Developed: 2026-01-28*  
*Version: 1.0*  
*Status: Production Ready âœ…*

---

**Need help? Start with README.md â†’ Everything is documented!**
